@article{chenSLAMOverviewSingle2022,
  title = {{{SLAM Overview}}: {{From Single Sensor}} to {{Heterogeneous Fusion}}},
  shorttitle = {{{SLAM Overview}}},
  author = {Chen, Weifeng and Zhou, Chengjun and Shang, Guangtao and Wang, Xiyang and Li, Zhenxiong and Xu, Chonghui and Hu, Kai},
  year = {2022},
  month = nov,
  journal = {Remote Sensing},
  volume = {14},
  number = {23},
  pages = {6033},
  issn = {2072-4292},
  doi = {10.3390/rs14236033},
  urldate = {2024-03-31},
  abstract = {After decades of development, LIDAR and visual SLAM technology has relatively matured and been widely used in the military and civil fields. SLAM technology enables the mobile robot to have the abilities of autonomous positioning and mapping, which allows the robot to move in indoor and outdoor scenes where GPS signals are scarce. However, SLAM technology relying only on a single sensor has its limitations. For example, LIDAR SLAM is not suitable for scenes with highly dynamic or sparse features, and visual SLAM has poor robustness in low-texture or dark scenes. However, through the fusion of the two technologies, they have great potential to learn from each other. Therefore, this paper predicts that SLAM technology combining LIDAR and visual sensors, as well as various other sensors, will be the mainstream direction in the future. This paper reviews the development history of SLAM technology, deeply analyzes the hardware information of LIDAR and cameras, and presents some classical open source algorithms and datasets. According to the algorithm adopted by the fusion sensor, the traditional multi-sensor fusion methods based on uncertainty, features, and novel deep learning are introduced in detail. The excellent performance of the multi-sensor fusion method in complex scenes is summarized, and the future development of multi-sensor fusion method is prospected.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/pontus/.local/share/zotero/storage/26UXXV85/Chen et al. - 2022 - SLAM Overview From Single Sensor to Heterogeneous.pdf}
}

@article{kAutonomousVehicleNavigation2022,
  title = {Autonomous {{Vehicle Navigation}} with {{LIDAR}} Using {{Path Planning}}},
  author = {K, Rahul M and B, Sumukh and Uppunda, Praveen L and Raju, Vinayaka and Gururaj, C},
  year = {2022},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2212.07155},
  urldate = {2024-03-29},
  abstract = {In this paper, a complete framework for Autonomous Self Driving is implemented. LIDAR, Camera and IMU sensors are used together. The entire data communication is managed using Robot Operating System which provides a robust platform for implementation of Robotics Projects. Jetson Nano is used to provide powerful on-board processing capabilities. Sensor fusion is performed on the data received from the different sensors to improve the accuracy of the decision making and inferences that we derive from the data. This data is then used to create a localized map of the environment. In this step, the position of the vehicle is obtained with respect to the Mapping done using the sensor data.The different SLAM techniques used for this purpose are Hector Mapping and GMapping which are widely used mapping techniques in ROS. Apart from SLAM that primarily uses LIDAR data, Visual Odometry is implemented using a Monocular Camera. The sensor fused data is then used by Adaptive Monte Carlo Localization for car localization. Using the localized map developed, Path Planning techniques like "TEB planner" and "Dynamic Window Approach" are implemented for autonomous navigation of the vehicle. The last step in the Project is the implantation of Control which is the final decision making block in the pipeline that gives speed and steering data for the navigation that is compatible with Ackermann Kinematics. The implementation of such a control block under a ROS framework using the three sensors, viz, LIDAR, Camera and IMU is a novel approach that is undertaken in this project.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Robotics (cs.RO),Systems and Control (eess.SY)},
  file = {/home/pontus/.local/share/zotero/storage/XQQ3E9DV/K et al. - 2022 - Autonomous Vehicle Navigation with LIDAR using Pat.pdf}
}

@inproceedings{kholeComprehensiveStudySimultaneous2023,
  title = {A {{Comprehensive Study}} on {{Simultaneous Localization}} and {{Mapping}} ({{SLAM}}): {{Types}}, {{Challenges}} and {{Applications}}},
  shorttitle = {A {{Comprehensive Study}} on {{Simultaneous Localization}} and {{Mapping}} ({{SLAM}})},
  booktitle = {2023 {{International Conference}} on {{Sustainable Computing}} and {{Smart Systems}} ({{ICSCSS}})},
  author = {Khole, Aneesh and Thakar, Atharva and Shende, Shreyas and Karajkhede, Varad},
  year = {2023},
  month = jun,
  pages = {643--650},
  publisher = {IEEE},
  address = {Coimbatore, India},
  doi = {10.1109/ICSCSS57650.2023.10169695},
  urldate = {2024-03-31},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350333602}
}

@article{placedSurveyActiveSimultaneous2022,
  title = {A {{Survey}} on {{Active Simultaneous Localization}} and {{Mapping}}: {{State}} of the {{Art}} and {{New Frontiers}}},
  shorttitle = {A {{Survey}} on {{Active Simultaneous Localization}} and {{Mapping}}},
  author = {Placed, Julio A. and Strader, Jared and Carrillo, Henry and Atanasov, Nikolay and Indelman, Vadim and Carlone, Luca and Castellanos, Jos{\'e} A.},
  year = {2022},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2207.00254},
  urldate = {2024-03-31},
  abstract = {Active Simultaneous Localization and Mapping (SLAM) is the problem of planning and controlling the motion of a robot to build the most accurate and complete model of the surrounding environment. Since the first foundational work in active perception appeared, more than three decades ago, this field has received increasing attention across different scientific communities. This has brought about many different approaches and formulations, and makes a review of the current trends necessary and extremely valuable for both new and experienced researchers. In this work, we survey the state-of-the-art in active SLAM and take an in-depth look at the open challenges that still require attention to meet the needs of modern applications. After providing a historical perspective, we present a unified problem formulation and review the well-established modular solution scheme, which decouples the problem into three stages that identify, select, and execute potential navigation actions. We then analyze alternative approaches, including belief-space planning and deep reinforcement learning techniques, and review related work on multi-robot coordination. The manuscript concludes with a discussion of new research directions, addressing reproducible research, active spatial perception, and practical applications, among other topics.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {FOS: Computer and information sciences,Robotics (cs.RO)}
}
